{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGp66Mj3MzrH"
      },
      "outputs": [],
      "source": [
        "#Import Section\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "import statistics\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('switerland with zeros.csv')\n",
        "df['target'] = np.where(df['target'] > 1, 1, np.where(df['target'] == 1, 0, df['target']))\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "RZw1ssrlQVyl",
        "outputId": "34414ecc-0fb3-4685-df24-e29362d5537a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
              "0     32    1   1         6     0    1        2        7      2       20   \n",
              "1     34    1   4         9     0    1        1       29      2        2   \n",
              "2     35    1   4         1     0    1        2        9      3        1   \n",
              "3     36    1   4         8     0    1        2        5      3       16   \n",
              "4     38    0   4         7     0    1        2       35      2       10   \n",
              "..   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
              "118   70    1   4         9     0    2        3       55      3        4   \n",
              "119   70    1   4        12     0    3        2       32      3       35   \n",
              "120   72    1   3        18     0    1        4       12      2       32   \n",
              "121   73    0   3        18     0    2        3       67      2        4   \n",
              "122   74    1   2        15     0    1        3        2      2       30   \n",
              "\n",
              "     slope  ca  thal  target  \n",
              "0        2   1     1       1  \n",
              "1        2   1     1       1  \n",
              "2        1   1     4       1  \n",
              "3        3   1     3       1  \n",
              "4        2   1     1       1  \n",
              "..     ...  ..   ...     ...  \n",
              "118      3   1     4       1  \n",
              "119      3   1     4       1  \n",
              "120      3   3     1       0  \n",
              "121      2   1     2       1  \n",
              "122      2   1     1       1  \n",
              "\n",
              "[123 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c4a6534-87d5-417a-8efc-6bb970d88c66\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>34</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>29</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>36</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>55</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>72</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>67</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>74</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>30</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>123 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c4a6534-87d5-417a-8efc-6bb970d88c66')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c4a6534-87d5-417a-8efc-6bb970d88c66 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c4a6534-87d5-417a-8efc-6bb970d88c66');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cf26f4a2-e592-413a-8f39-33a637a366cd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cf26f4a2-e592-413a-8f39-33a637a366cd')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cf26f4a2-e592-413a-8f39-33a637a366cd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting into features and class label\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X = df.drop('target',axis = 1)\n",
        "y = df['target']\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state = 23)\n",
        "scaler  = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "qxRS6rCIVWiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "h8sIaCrhVJPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "m1 = 'knn classifier'\n",
        "k = 10  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=23)\n",
        "\n",
        "total_accuracy = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=9)\n",
        "    knn.fit(X_train_fold, y_train_fold)\n",
        "    knnpred = knn.predict(X_test_fold)\n",
        "    cm = confusion_matrix(y_test_fold, knnpred)\n",
        "    fold_accuracy = accuracy_score(y_test_fold, knnpred)*100\n",
        "    print('Accuracy for this fold:', fold_accuracy)\n",
        "    total_accuracy += fold_accuracy\n",
        "    print(\"Confusion Matrix for Fold:\")\n",
        "    print(cm)\n",
        "\n",
        "knnaccuracy = total_accuracy / k\n",
        "print(\"Mean Accuracy =\", knnaccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cb7Ku6fVK1h",
        "outputId": "530d2c30-6a53-4890-87d9-10623155dd1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[13]]\n",
            "Accuracy for this fold: 92.3076923076923\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 12]]\n",
            "Accuracy for this fold: 84.61538461538461\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 10]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Mean Accuracy = 93.52564102564102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
        "m2 = 'lr classifier'\n",
        "\n",
        "k = 10  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=23)\n",
        "\n",
        "total_accuracy = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = train_test_split(X, test_size=0.3, random_state=23)\n",
        "    y_train_fold, y_test_fold = train_test_split(y, test_size=0.3, random_state=23)\n",
        "\n",
        "\n",
        "    lr = LogisticRegression()\n",
        "    lr.fit(X_train_fold,y_train_fold)\n",
        "    lrpred = lr.predict(X_test_fold)\n",
        "    cm = confusion_matrix(y_test_fold, lrpred)\n",
        "    fold_accuracy = accuracy_score(y_test_fold, lrpred) * 100\n",
        "    print('Accuracy for this fold:', fold_accuracy)\n",
        "    total_accuracy += fold_accuracy\n",
        "    print(\"Confusion Matrix for Fold:\")\n",
        "    print(cm)\n",
        "\n",
        "lraccuracy = total_accuracy / k\n",
        "print(\"Mean Accuracy =\", lraccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEONcGdTbm8M",
        "outputId": "264acc46-6113-412e-9741-89f2fa7db2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Accuracy for this fold: 91.8918918918919\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  3]\n",
            " [ 0 34]]\n",
            "Mean Accuracy = 91.89189189189189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "m3 = 'svm classifier'\n",
        "k = 10  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=0)\n",
        "\n",
        "total_accuracy = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    Svm = SVC(kernel = 'linear',C=1)\n",
        "    Svm.fit(X_train_fold,y_train_fold)\n",
        "    Svmpred = Svm.predict(X_test_fold)\n",
        "    cm = confusion_matrix(y_test_fold, Svmpred)\n",
        "    fold_accuracy = accuracy_score(y_test_fold, Svmpred) * 100\n",
        "    print('Accuracy for this fold:', fold_accuracy)\n",
        "    total_accuracy += fold_accuracy\n",
        "    print(\"Confusion Matrix for Fold:\")\n",
        "    print(cm)\n",
        "\n",
        "svmaccuracy = total_accuracy / k\n",
        "print(\"Mean Accuracy =\", svmaccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7jeCqoqbo2p",
        "outputId": "255cfd99-590e-4988-efae-cd3ebe54927e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[13]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[13]]\n",
            "Accuracy for this fold: 84.61538461538461\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 10]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 1 10]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Mean Accuracy = 92.62820512820512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DT\n",
        "m4 = 'decision tree'\n",
        "k = 10  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=23)\n",
        "\n",
        "total_accuracy = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    dt = DecisionTreeClassifier(criterion='entropy',random_state = 0,max_depth=6)\n",
        "    dt.fit(X_train_fold,y_train_fold)\n",
        "    dtpred = dt.predict(X_test_fold)\n",
        "    cm = confusion_matrix(y_test_fold, dtpred)\n",
        "    fold_accuracy = accuracy_score(y_test_fold, dtpred) * 100\n",
        "    print('Accuracy for this fold:', fold_accuracy)\n",
        "    total_accuracy += fold_accuracy\n",
        "    print(\"Confusion Matrix for Fold:\")\n",
        "    print(cm)\n",
        "\n",
        "dtaccuracy = total_accuracy / k\n",
        "print(\"Mean Accuracy =\", dtaccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftsAjjI3bqTU",
        "outputId": "dd764258-a42d-42bc-d758-8f6282f18f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this fold: 76.92307692307693\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 3 10]]\n",
            "Accuracy for this fold: 69.23076923076923\n",
            "Confusion Matrix for Fold:\n",
            "[[0 1]\n",
            " [3 9]]\n",
            "Accuracy for this fold: 84.61538461538461\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 1 11]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 1 10]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 2 10]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 2 10]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 1  1]\n",
            " [ 0 10]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 1 10]]\n",
            "Mean Accuracy = 83.91025641025642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GaussianNB\n",
        "\n",
        "m5 = 'gaussian nb'\n",
        "k = 10  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=23)\n",
        "\n",
        "total_accuracy = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    nb = GaussianNB()\n",
        "    nb.fit(X_train_fold,y_train_fold)\n",
        "    nbpred = nb.predict(X_test_fold)\n",
        "    cm = confusion_matrix(y_test_fold, nbpred)\n",
        "    fold_accuracy = accuracy_score(y_test_fold, nbpred) * 100\n",
        "    print('Accuracy for this fold:', fold_accuracy)\n",
        "    total_accuracy += fold_accuracy\n",
        "    print(\"Confusion Matrix for Fold:\")\n",
        "    print(cm)\n",
        "\n",
        "nbaccuracy = total_accuracy / k\n",
        "print(\"Mean Accuracy =\", nbaccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQT-OgRubtWL",
        "outputId": "a0ca3b0e-8fcf-483c-b7d5-27f633cc57f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this fold: 53.84615384615385\n",
            "Confusion Matrix for Fold:\n",
            "[[0 0]\n",
            " [6 7]]\n",
            "Accuracy for this fold: 30.76923076923077\n",
            "Confusion Matrix for Fold:\n",
            "[[0 1]\n",
            " [8 4]]\n",
            "Accuracy for this fold: 15.384615384615385\n",
            "Confusion Matrix for Fold:\n",
            "[[ 1  1]\n",
            " [10  1]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 2 10]]\n",
            "Accuracy for this fold: 50.0\n",
            "Confusion Matrix for Fold:\n",
            "[[0 1]\n",
            " [5 6]]\n",
            "Accuracy for this fold: 41.66666666666667\n",
            "Confusion Matrix for Fold:\n",
            "[[0 0]\n",
            " [7 5]]\n",
            "Accuracy for this fold: 66.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[0 0]\n",
            " [4 8]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[1 0]\n",
            " [2 9]]\n",
            "Accuracy for this fold: 41.66666666666667\n",
            "Confusion Matrix for Fold:\n",
            "[[2 0]\n",
            " [7 3]]\n",
            "Accuracy for this fold: 75.0\n",
            "Confusion Matrix for Fold:\n",
            "[[1 0]\n",
            " [3 8]]\n",
            "Mean Accuracy = 54.16666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "m6 = 'qda classifier'\n",
        "\n",
        "k = 10  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "total_accuracy = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    qda = QuadraticDiscriminantAnalysis()\n",
        "    qda.fit(X_train_fold, y_train_fold)\n",
        "    qdapred = qda.predict(X_test_fold)\n",
        "    cm = confusion_matrix(y_test_fold, qdapred)\n",
        "    fold_accuracy = accuracy_score(y_test_fold, qdapred) * 100\n",
        "    print('Accuracy for this fold:', fold_accuracy)\n",
        "    total_accuracy += fold_accuracy\n",
        "    print(\"Confusion Matrix for Fold:\")\n",
        "    print(cm)\n",
        "\n",
        "qdaaccuracy = total_accuracy / k\n",
        "print(\"Mean Accuracy =\", qdaaccuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W5PswTZbutj",
        "outputId": "629562a5-bd21-488f-a5aa-d8dedd1d3328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this fold: 84.61538461538461\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[13]]\n",
            "Accuracy for this fold: 92.3076923076923\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 12]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 10]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Mean Accuracy = 93.52564102564102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "m7 = 'rf classifier'\n",
        "k = 10  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=23)\n",
        "\n",
        "total_accuracy = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    rf = RandomForestClassifier(n_estimators=5, random_state=2,max_depth=5)\n",
        "    rf.fit(X_train_fold, y_train_fold)\n",
        "    rfpred = rf.predict(X_test_fold)\n",
        "    cm = confusion_matrix(y_test_fold, rfpred)\n",
        "    fold_accuracy = accuracy_score(y_test_fold, rfpred) * 100\n",
        "    print('Accuracy for this fold:', fold_accuracy)\n",
        "    total_accuracy += fold_accuracy\n",
        "    print(\"Confusion Matrix for Fold:\")\n",
        "    print(cm)\n",
        "\n",
        "rfaccuracy = total_accuracy / k\n",
        "print(\"Mean Accuracy =\", rfaccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQ7PD6Tnbw6A",
        "outputId": "6a6c53d9-c58c-4d7f-8229-50be707a80e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[13]]\n",
            "Accuracy for this fold: 92.3076923076923\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 12]]\n",
            "Accuracy for this fold: 84.61538461538461\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 1 10]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 1 11]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 10]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 0 11]]\n",
            "Mean Accuracy = 91.85897435897435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m8 = 'adaboost classifier'\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "k = 10  # Number of folds\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=23)\n",
        "\n",
        "total_accuracy = 0\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    base_classifier = DecisionTreeClassifier(max_depth=4)\n",
        "    adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=20, random_state=23)\n",
        "    adaboost_classifier.fit(X_train_fold, y_train_fold)\n",
        "    boostpred = adaboost_classifier.predict(X_test_fold)\n",
        "    cm = confusion_matrix(y_test_fold, boostpred)\n",
        "    fold_accuracy = accuracy_score(y_test_fold, boostpred) * 100\n",
        "    print('Accuracy for this fold:', fold_accuracy)\n",
        "    total_accuracy += fold_accuracy\n",
        "    print(\"Confusion Matrix for Fold:\")\n",
        "    print(cm)\n",
        "\n",
        "boostaccuracy = total_accuracy / k\n",
        "print(\"Mean Accuracy =\", boostaccuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cgj9LVQkbyQ7",
        "outputId": "ccac3115-f23b-46ec-880b-25bd1a0ab59a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this fold: 84.61538461538461\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 2 11]]\n",
            "Accuracy for this fold: 84.61538461538461\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 1 11]]\n",
            "Accuracy for this fold: 84.61538461538461\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 11]]\n",
            "Accuracy for this fold: 100.0\n",
            "Confusion Matrix for Fold:\n",
            "[[12]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 1 10]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 1 11]]\n",
            "Accuracy for this fold: 91.66666666666666\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  0]\n",
            " [ 1 11]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 1 10]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  2]\n",
            " [ 0 10]]\n",
            "Accuracy for this fold: 83.33333333333334\n",
            "Confusion Matrix for Fold:\n",
            "[[ 0  1]\n",
            " [ 1 10]]\n",
            "Mean Accuracy = 87.05128205128206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning Model"
      ],
      "metadata": {
        "id": "E5QvMKuuXFv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrjHF37nXIVg",
        "outputId": "106bcc5b-eaaa-4ba3-9b1c-2a43b280111b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.3/612.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.22.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "input_layer = Input(shape=(X_train.shape[1],))\n",
        "d1 = Dense(units=200, activation='relu')(input_layer)\n",
        "d2 = Dense(units=100, activation='relu')(d1)\n",
        "d3 = Dense(units=100, activation='relu')(d2)\n",
        "d4 = Dense(units=100, activation='relu')(d3)\n",
        "d5 = Dense(units=100, activation='relu')(d4)\n",
        "d6 = Dense(units=100, activation='relu')(d5)\n",
        "d7 = Dense(units=100, activation='relu')(d6)\n",
        "d8 = Dense(units=100, activation='relu')(d7)\n",
        "# d9 = Dense(units=100, activation='relu')(d8)\n",
        "output_layer = Dense(units=1, activation='sigmoid')(d8)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model with metrics and optimizer\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', tf.keras.metrics.Recall(name=\"Sensitivity\"),\n",
        "             tf.keras.metrics.SpecificityAtSensitivity(0.5, name=\"Specificity\"),\n",
        "             tfa.metrics.F1Score(num_classes=1, threshold=0.5)],\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(x=X_train, y=y_train, batch_size=100, epochs=400)\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "test_loss, test_accuracy, sensitivity, specificity, f1_score = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "print(f\"Sensitivity (Recall): {sensitivity:.2f}\")\n",
        "print(f\"Specificity: {specificity:.2f}\")\n",
        "print(\"x=\" + str(list(map('{:.2f}%'.format,f1_score))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxVbosQKXNBk",
        "outputId": "5386de72-649f-4f08-a5fa-d6ba8a75f436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.6990 - accuracy: 0.1279 - Sensitivity: 0.0741 - Specificity: 0.2000 - f1_score: 0.1379\n",
            "Epoch 2/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6986 - accuracy: 0.1279 - Sensitivity: 0.0741 - Specificity: 0.2000 - f1_score: 0.1379\n",
            "Epoch 3/400\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6983 - accuracy: 0.1744 - Sensitivity: 0.1235 - Specificity: 0.2000 - f1_score: 0.2198\n",
            "Epoch 4/400\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6980 - accuracy: 0.1744 - Sensitivity: 0.1235 - Specificity: 0.2000 - f1_score: 0.2198\n",
            "Epoch 5/400\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6977 - accuracy: 0.1744 - Sensitivity: 0.1235 - Specificity: 1.0000 - f1_score: 0.2198\n",
            "Epoch 6/400\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6974 - accuracy: 0.1860 - Sensitivity: 0.1358 - Specificity: 0.6000 - f1_score: 0.2391\n",
            "Epoch 7/400\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6970 - accuracy: 0.2093 - Sensitivity: 0.1605 - Specificity: 0.6000 - f1_score: 0.2766\n",
            "Epoch 8/400\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6967 - accuracy: 0.2209 - Sensitivity: 0.1728 - Specificity: 0.2000 - f1_score: 0.2947\n",
            "Epoch 9/400\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6964 - accuracy: 0.2791 - Sensitivity: 0.2346 - Specificity: 0.2000 - f1_score: 0.3800\n",
            "Epoch 10/400\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.6961 - accuracy: 0.2907 - Sensitivity: 0.2469 - Specificity: 0.2000 - f1_score: 0.3960\n",
            "Epoch 11/400\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6958 - accuracy: 0.3140 - Sensitivity: 0.2716 - Specificity: 0.2000 - f1_score: 0.4272\n",
            "Epoch 12/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6955 - accuracy: 0.3372 - Sensitivity: 0.2963 - Specificity: 0.2000 - f1_score: 0.4571\n",
            "Epoch 13/400\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.6952 - accuracy: 0.3488 - Sensitivity: 0.3086 - Specificity: 0.2000 - f1_score: 0.4717\n",
            "Epoch 14/400\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.6949 - accuracy: 0.3488 - Sensitivity: 0.3086 - Specificity: 0.2000 - f1_score: 0.4717\n",
            "Epoch 15/400\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6945 - accuracy: 0.3488 - Sensitivity: 0.3086 - Specificity: 0.2000 - f1_score: 0.4717\n",
            "Epoch 16/400\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6942 - accuracy: 0.3837 - Sensitivity: 0.3457 - Specificity: 0.2000 - f1_score: 0.5138\n",
            "Epoch 17/400\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.6939 - accuracy: 0.4302 - Sensitivity: 0.3951 - Specificity: 0.2000 - f1_score: 0.5664\n",
            "Epoch 18/400\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.6936 - accuracy: 0.4651 - Sensitivity: 0.4321 - Specificity: 0.2000 - f1_score: 0.6034\n",
            "Epoch 19/400\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6933 - accuracy: 0.4767 - Sensitivity: 0.4444 - Specificity: 0.2000 - f1_score: 0.6154\n",
            "Epoch 20/400\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.6930 - accuracy: 0.5000 - Sensitivity: 0.4691 - Specificity: 0.2000 - f1_score: 0.6387\n",
            "Epoch 21/400\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6927 - accuracy: 0.5465 - Sensitivity: 0.5185 - Specificity: 0.2000 - f1_score: 0.6829\n",
            "Epoch 22/400\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6924 - accuracy: 0.5930 - Sensitivity: 0.5679 - Specificity: 0.2000 - f1_score: 0.7244\n",
            "Epoch 23/400\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6920 - accuracy: 0.6163 - Sensitivity: 0.5926 - Specificity: 0.2000 - f1_score: 0.7442\n",
            "Epoch 24/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6917 - accuracy: 0.6279 - Sensitivity: 0.6173 - Specificity: 0.2000 - f1_score: 0.7576\n",
            "Epoch 25/400\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6914 - accuracy: 0.6395 - Sensitivity: 0.6296 - Specificity: 0.2000 - f1_score: 0.7669\n",
            "Epoch 26/400\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6911 - accuracy: 0.6512 - Sensitivity: 0.6420 - Specificity: 0.2000 - f1_score: 0.7761\n",
            "Epoch 27/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6908 - accuracy: 0.6860 - Sensitivity: 0.6790 - Specificity: 0.2000 - f1_score: 0.8029\n",
            "Epoch 28/400\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6905 - accuracy: 0.6977 - Sensitivity: 0.7037 - Specificity: 0.2000 - f1_score: 0.8143\n",
            "Epoch 29/400\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6902 - accuracy: 0.7326 - Sensitivity: 0.7407 - Specificity: 0.2000 - f1_score: 0.8392\n",
            "Epoch 30/400\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6899 - accuracy: 0.7558 - Sensitivity: 0.7654 - Specificity: 0.2000 - f1_score: 0.8552\n",
            "Epoch 31/400\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.6896 - accuracy: 0.7907 - Sensitivity: 0.8025 - Specificity: 0.2000 - f1_score: 0.8784\n",
            "Epoch 32/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6892 - accuracy: 0.7907 - Sensitivity: 0.8148 - Specificity: 0.2000 - f1_score: 0.8800\n",
            "Epoch 33/400\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6889 - accuracy: 0.8023 - Sensitivity: 0.8272 - Specificity: 0.2000 - f1_score: 0.8874\n",
            "Epoch 34/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6886 - accuracy: 0.8140 - Sensitivity: 0.8395 - Specificity: 0.2000 - f1_score: 0.8947\n",
            "Epoch 35/400\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6883 - accuracy: 0.8256 - Sensitivity: 0.8519 - Specificity: 0.2000 - f1_score: 0.9020\n",
            "Epoch 36/400\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6880 - accuracy: 0.8256 - Sensitivity: 0.8519 - Specificity: 0.2000 - f1_score: 0.9020\n",
            "Epoch 37/400\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6877 - accuracy: 0.8256 - Sensitivity: 0.8519 - Specificity: 1.0000 - f1_score: 0.9020\n",
            "Epoch 38/400\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6874 - accuracy: 0.8256 - Sensitivity: 0.8519 - Specificity: 1.0000 - f1_score: 0.9020\n",
            "Epoch 39/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6871 - accuracy: 0.8256 - Sensitivity: 0.8519 - Specificity: 1.0000 - f1_score: 0.9020\n",
            "Epoch 40/400\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.6868 - accuracy: 0.8605 - Sensitivity: 0.8889 - Specificity: 1.0000 - f1_score: 0.9231\n",
            "Epoch 41/400\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6865 - accuracy: 0.8721 - Sensitivity: 0.9012 - Specificity: 0.8000 - f1_score: 0.9299\n",
            "Epoch 42/400\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6862 - accuracy: 0.8837 - Sensitivity: 0.9136 - Specificity: 0.8000 - f1_score: 0.9367\n",
            "Epoch 43/400\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6858 - accuracy: 0.8953 - Sensitivity: 0.9259 - Specificity: 0.8000 - f1_score: 0.9434\n",
            "Epoch 44/400\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6855 - accuracy: 0.8953 - Sensitivity: 0.9259 - Specificity: 0.8000 - f1_score: 0.9434\n",
            "Epoch 45/400\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6852 - accuracy: 0.9070 - Sensitivity: 0.9506 - Specificity: 0.8000 - f1_score: 0.9506\n",
            "Epoch 46/400\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6849 - accuracy: 0.9302 - Sensitivity: 0.9753 - Specificity: 0.8000 - f1_score: 0.9634\n",
            "Epoch 47/400\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6846 - accuracy: 0.9302 - Sensitivity: 0.9753 - Specificity: 0.8000 - f1_score: 0.9634\n",
            "Epoch 48/400\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6843 - accuracy: 0.9302 - Sensitivity: 0.9753 - Specificity: 0.8000 - f1_score: 0.9634\n",
            "Epoch 49/400\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6840 - accuracy: 0.9302 - Sensitivity: 0.9753 - Specificity: 0.8000 - f1_score: 0.9634\n",
            "Epoch 50/400\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6837 - accuracy: 0.9302 - Sensitivity: 0.9753 - Specificity: 0.6000 - f1_score: 0.9634\n",
            "Epoch 51/400\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.6834 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.6000 - f1_score: 0.9697\n",
            "Epoch 52/400\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6831 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.6000 - f1_score: 0.9697\n",
            "Epoch 53/400\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6827 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.6000 - f1_score: 0.9697\n",
            "Epoch 54/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6824 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.6000 - f1_score: 0.9697\n",
            "Epoch 55/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6821 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.6000 - f1_score: 0.9697\n",
            "Epoch 56/400\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6818 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.6000 - f1_score: 0.9697\n",
            "Epoch 57/400\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6815 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.4000 - f1_score: 0.9697\n",
            "Epoch 58/400\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6812 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.4000 - f1_score: 0.9697\n",
            "Epoch 59/400\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.6808 - accuracy: 0.9419 - Sensitivity: 0.9877 - Specificity: 0.4000 - f1_score: 0.9697\n",
            "Epoch 60/400\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6805 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 0.4000 - f1_score: 0.9759\n",
            "Epoch 61/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6802 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 0.4000 - f1_score: 0.9759\n",
            "Epoch 62/400\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6799 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 0.4000 - f1_score: 0.9759\n",
            "Epoch 63/400\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6795 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 0.4000 - f1_score: 0.9759\n",
            "Epoch 64/400\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6792 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 0.4000 - f1_score: 0.9759\n",
            "Epoch 65/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6789 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 0.4000 - f1_score: 0.9759\n",
            "Epoch 66/400\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6786 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9759\n",
            "Epoch 67/400\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.6782 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9759\n",
            "Epoch 68/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6779 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9759\n",
            "Epoch 69/400\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6776 - accuracy: 0.9535 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9759\n",
            "Epoch 70/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6772 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 71/400\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.6769 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 72/400\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.6765 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 73/400\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.6762 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 74/400\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.6758 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 75/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6755 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 76/400\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6752 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 77/400\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.6748 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 78/400\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6744 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 79/400\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.6741 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 80/400\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.6737 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 81/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6734 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 82/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6730 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 83/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6726 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 84/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6723 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 85/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6719 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 86/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6715 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 87/400\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6711 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 88/400\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.6708 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 89/400\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6704 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 90/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6700 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 91/400\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.6696 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 92/400\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6692 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 93/400\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6688 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 94/400\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6684 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 95/400\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6680 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 96/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6676 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 97/400\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6672 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 98/400\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6668 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 99/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6664 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 100/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6660 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 101/400\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6656 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 102/400\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.6652 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 103/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6648 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 104/400\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.6643 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 105/400\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.6639 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 106/400\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6635 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 107/400\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6631 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 108/400\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6626 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 109/400\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6622 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 110/400\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.6617 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 111/400\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6613 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 112/400\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.6608 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 113/400\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6604 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 114/400\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6599 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 115/400\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6595 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 116/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6590 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 117/400\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6585 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 118/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6581 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 119/400\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.6576 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 120/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6571 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 121/400\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.6567 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 122/400\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.6562 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 123/400\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.6557 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 124/400\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.6552 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 125/400\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.6547 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 126/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.6542 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 127/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.6537 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 128/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6532 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 129/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6527 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 0.8000 - f1_score: 0.9701\n",
            "Epoch 130/400\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6522 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 131/400\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.6517 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 132/400\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6511 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 133/400\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.6506 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 134/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6501 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 135/400\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.6495 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 136/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6490 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 137/400\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.6485 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 138/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6479 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 139/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6474 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 140/400\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.6468 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 141/400\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.6462 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 142/400\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.6457 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 143/400\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.6451 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 144/400\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 0.6445 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 145/400\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.6440 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 146/400\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.6434 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 147/400\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6428 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 148/400\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.6422 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 149/400\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.6416 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 150/400\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.6410 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 151/400\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.6404 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 152/400\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.6398 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 153/400\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.6392 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 154/400\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.6386 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 155/400\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.6380 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 156/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6374 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 157/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6367 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 158/400\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.6361 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 159/400\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.6355 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 160/400\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.6348 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 161/400\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.6342 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 162/400\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.6336 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 163/400\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.6329 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 164/400\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.6323 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 165/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6316 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 166/400\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.6309 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 167/400\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.6303 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 168/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6296 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 169/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6289 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 170/400\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.6282 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 171/400\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.6276 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 172/400\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.6269 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 173/400\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6262 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 174/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.6255 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 175/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6248 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 176/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6241 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 177/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6233 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 178/400\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.6226 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 179/400\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6219 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 180/400\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.6212 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 181/400\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6204 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 182/400\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.6197 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 183/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6190 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 184/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6182 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 185/400\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6174 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 186/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6167 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 187/400\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.6159 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 188/400\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6151 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 189/400\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.6143 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 190/400\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.6136 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 191/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6128 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 192/400\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.6120 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 193/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.6112 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 194/400\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.6104 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 195/400\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.6096 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 196/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.6087 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 197/400\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.6079 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 198/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.6071 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 199/400\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6062 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 200/400\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.6054 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 201/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.6045 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 202/400\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.6037 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 203/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.6028 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 204/400\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.6020 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 205/400\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.6011 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 206/400\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.6002 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 207/400\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.5993 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 208/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5984 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 209/400\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.5975 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 210/400\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.5966 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 211/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5957 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 212/400\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.5948 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 213/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5938 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 214/400\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5929 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 215/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5920 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 216/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5910 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 217/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5901 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 218/400\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.5891 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 219/400\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.5881 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 220/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5872 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 221/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5862 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 222/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5852 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 223/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5842 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 224/400\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.5832 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 225/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5822 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 226/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5812 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 227/400\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5801 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 228/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5791 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 229/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5781 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 230/400\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.5770 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 231/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5760 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 232/400\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.5749 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 233/400\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.5739 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 234/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5728 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 235/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5717 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 236/400\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 0.5706 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 237/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5695 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 238/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5684 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 239/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.5673 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 240/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5662 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 241/400\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.5651 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 242/400\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.5640 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 243/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5628 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 244/400\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.5617 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 245/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.5605 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 246/400\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.5594 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 247/400\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.5582 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 248/400\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.5570 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 249/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5559 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 250/400\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.5547 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 251/400\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.5535 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 252/400\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.5523 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 253/400\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5511 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 254/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5498 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 255/400\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.5486 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 256/400\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.5474 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 257/400\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.5461 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 258/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5449 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 259/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.5436 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 260/400\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.5424 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 261/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5411 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 262/400\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.5398 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 263/400\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5385 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 264/400\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 0.5372 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 265/400\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.5359 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 266/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5346 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 267/400\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.5333 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 268/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.5320 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 269/400\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.5306 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 270/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5293 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 271/400\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.5279 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 272/400\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.5266 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 273/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5252 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 274/400\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 0.5239 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 275/400\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5225 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 276/400\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 0.5211 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 277/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.5197 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 278/400\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.5183 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 279/400\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.5169 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 280/400\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.5154 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 281/400\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.5140 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 282/400\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.5126 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 283/400\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.5111 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 284/400\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.5097 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 285/400\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.5082 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 286/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5068 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 287/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.5053 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 288/400\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.5038 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 289/400\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.5023 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 290/400\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.5008 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 291/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4993 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 292/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4978 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 293/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4963 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 294/400\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.4948 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 295/400\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4932 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 296/400\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.4917 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 297/400\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.4902 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 298/400\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.4886 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 299/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.4871 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 300/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4855 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 301/400\n",
            "1/1 [==============================] - 0s 101ms/step - loss: 0.4840 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 302/400\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 0.4824 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 303/400\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4809 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 304/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.4793 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 305/400\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.4777 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 306/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.4761 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 307/400\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.4746 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 308/400\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.4730 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 309/400\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4714 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 310/400\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.4698 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 311/400\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4682 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 312/400\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.4666 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 313/400\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4649 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 314/400\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.4633 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 315/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4617 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 316/400\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4601 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 317/400\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.4585 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 318/400\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.4568 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 319/400\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.4552 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 320/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.4536 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 321/400\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.4519 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 322/400\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.4503 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 323/400\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.4487 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 324/400\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.4470 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 325/400\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.4454 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 326/400\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.4437 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 327/400\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.4421 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 328/400\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.4404 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 329/400\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4388 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 330/400\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.4371 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 331/400\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.4354 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 332/400\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.4338 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 333/400\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.4321 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 334/400\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.4304 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 335/400\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.4288 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 336/400\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.4271 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 337/400\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.4254 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 338/400\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.4238 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 339/400\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.4221 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 340/400\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.4204 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 341/400\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.4188 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 342/400\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.4171 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 343/400\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.4154 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 344/400\n",
            "1/1 [==============================] - 0s 62ms/step - loss: 0.4137 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 345/400\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 0.4121 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 346/400\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4104 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 347/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.4087 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 348/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.4071 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 349/400\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.4054 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 350/400\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.4037 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 351/400\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.4021 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 352/400\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.4004 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 353/400\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3987 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 354/400\n",
            "1/1 [==============================] - 0s 66ms/step - loss: 0.3971 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 355/400\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3954 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 356/400\n",
            "1/1 [==============================] - 0s 64ms/step - loss: 0.3938 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 357/400\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3921 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 358/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3904 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 359/400\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.3888 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 360/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3871 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 361/400\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3855 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 362/400\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.3838 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 363/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3822 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 364/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3806 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 365/400\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.3789 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 366/400\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.3773 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 367/400\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.3757 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 368/400\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.3740 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 369/400\n",
            "1/1 [==============================] - 0s 47ms/step - loss: 0.3724 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 370/400\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3708 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 371/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3692 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 372/400\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.3676 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 373/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3659 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 374/400\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.3643 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 375/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3627 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 376/400\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.3611 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 377/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3595 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 378/400\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.3579 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 379/400\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.3563 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 380/400\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 0.3548 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 381/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3532 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 382/400\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 0.3516 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 383/400\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 0.3500 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 384/400\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3485 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 385/400\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.3469 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 386/400\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 0.3454 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 387/400\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.3438 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 388/400\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3423 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 389/400\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.3407 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 390/400\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 0.3392 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 391/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.3377 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 392/400\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 0.3362 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 393/400\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 0.3347 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 394/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3332 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 395/400\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.3317 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 396/400\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3302 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 397/400\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3287 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 398/400\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.3272 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 399/400\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.3258 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "Epoch 400/400\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3243 - accuracy: 0.9419 - Sensitivity: 1.0000 - Specificity: 1.0000 - f1_score: 0.9701\n",
            "2/2 [==============================] - 1s 50ms/step - loss: 0.4027 - accuracy: 0.9189 - Sensitivity: 1.0000 - Specificity: 0.3333 - f1_score: 0.9577\n",
            "Test Accuracy: 91.89%\n",
            "Sensitivity (Recall): 1.00\n",
            "Specificity: 0.33\n",
            "x=['0.96%']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['Logistic Regression', 'Naive Bayes', 'Random Forest','adaboost classifier', 'K-Nearest Neighbour', 'Decision Tree', 'Support Vector Machine','Deep Learning Model ']\n",
        "accuracy_scores = [lraccuracy,nbaccuracy,rfaccuracy,boostaccuracy,knnaccuracy,dtaccuracy,svmaccuracy,test_accuracy*100]\n",
        "model_ev = pd.DataFrame({'Model': model_names, 'Accuracy': accuracy_scores})\n",
        "model_ev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "lyG5o0-nXsnW",
        "outputId": "821bd62f-93d3-4b7c-96f7-09f5e9cd3385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    Model   Accuracy\n",
              "0     Logistic Regression  91.891892\n",
              "1             Naive Bayes  54.166667\n",
              "2           Random Forest  91.858974\n",
              "3     adaboost classifier  87.051282\n",
              "4     K-Nearest Neighbour  93.525641\n",
              "5           Decision Tree  83.910256\n",
              "6  Support Vector Machine  92.628205\n",
              "7    Deep Learning Model   91.891891"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55304327-1ddd-47f8-8407-647bf85c8740\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Logistic Regression</td>\n",
              "      <td>91.891892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>54.166667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>91.858974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>adaboost classifier</td>\n",
              "      <td>87.051282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>K-Nearest Neighbour</td>\n",
              "      <td>93.525641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Decision Tree</td>\n",
              "      <td>83.910256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>92.628205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Deep Learning Model</td>\n",
              "      <td>91.891891</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55304327-1ddd-47f8-8407-647bf85c8740')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55304327-1ddd-47f8-8407-647bf85c8740 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55304327-1ddd-47f8-8407-647bf85c8740');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fddbe725-e5e3-455c-a106-65ff60b64d71\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fddbe725-e5e3-455c-a106-65ff60b64d71')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fddbe725-e5e3-455c-a106-65ff60b64d71 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['red', 'orange', 'yellow', 'green', 'blue', 'gold', 'orange', ]\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.title(\"Barchart Accuracy of Different ML Models\")\n",
        "plt.xlabel(\"Algorithms\")\n",
        "plt.ylabel(\"% Accuracy\")\n",
        "plt.bar(model_ev ['Model'], model_ev['Accuracy'], color = colors)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "GlpIIm97YCCE",
        "outputId": "54ee7dee-c670-4c76-f743-062077329c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}